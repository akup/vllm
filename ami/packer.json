{
  "variables": {
    "aws_region": "us-east-1",
    "source_image": "",
    "instance_type": "g5.xlarge",
    "ami_name": "project-vllm-modified-ami-{{timestamp}}",
    "ami_description": "Self-contained AMI with vLLM + PoC backend (native install, no Docker), for MLNode pow_v2 backends",
    "python_version": "3.11",
    "vllm_build_cache_bucket": "",
    "vllm_build_cache_prefix": "vllm-wheels",
    "vllm_build_cache_version": "",
    "vllm_build_overlay_files": "",
    "vllm_build_version": "0.9.1",
    "vllm_build_verbose": "1",
    "iam_instance_profile": ""
  },
  "builders": [
    {
      "type": "amazon-ebs",
      "region": "{{user `aws_region`}}",
      "iam_instance_profile": "{{user `iam_instance_profile`}}",
      "source_ami_filter": {
        "filters": {
          "virtualization-type": "hvm",
          "name": "Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.9 (Amazon Linux 2023)*",
          "root-device-type": "ebs"
        },
        "owners": ["amazon"],
        "most_recent": true
      },
      "source_ami": "{{user `source_image`}}",
      "instance_type": "{{user `instance_type`}}",
      "ssh_username": "ec2-user",
      "ssh_pty": true,
      "ami_name": "{{user `ami_name`}}",
      "ami_description": "{{user `ami_description`}}",
      "launch_block_device_mappings": [
        {
          "device_name": "/dev/xvda",
          "volume_size": 100,
          "volume_type": "gp3",
          "delete_on_termination": true
        }
      ],
      "tags": {
        "Name": "{{user `ami_name`}}",
        "Source": "vLLM native + PoC overlay",
        "Project": "project-vllm-modified"
      },
      "ami_regions": ["{{user `aws_region`}}"],
      "skip_region_validation": false,
      "skip_save_build_region": false
    }
  ],
  "provisioners": [
    {
      "type": "shell",
      "inline": [
        "echo 'Updating system packages...'",
        "sudo dnf update -y",
        "sudo dnf clean all",
        "echo 'System update completed'"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Installing CUDA 12.x toolkit for vLLM build (vLLM expects CUDA 12, not 13)...'",
        "sudo dnf install -y dnf-plugins-core",
        "sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo || true",
        "sudo dnf clean all",
        "sudo dnf install -y cuda-toolkit-12-1 || sudo dnf install -y cuda-12-1 || true",
        "sudo dnf install -y cuda-nvtx-12-1 cuda-nvtx-devel-12-1 || sudo dnf install -y cuda-nvtx cuda-nvtx-devel || true",
        "VLLM_CUDA_DIR=''",
        "for d in /usr/local/cuda-12.1 /usr/local/cuda-12.2 /usr/local/cuda-12.4 /usr/local/cuda-12; do [ -x \"$d/bin/nvcc\" ] 2>/dev/null && VLLM_CUDA_DIR=$d && break; done",
        "[ -z \"$VLLM_CUDA_DIR\" ] && for d in /usr/local/cuda-12*; do [ -x \"$d/bin/nvcc\" ] 2>/dev/null && VLLM_CUDA_DIR=$d && break; done",
        "[ -n \"$VLLM_CUDA_DIR\" ] && echo \"Using CUDA for vLLM build: $VLLM_CUDA_DIR\" && $VLLM_CUDA_DIR/bin/nvcc --version || echo 'WARNING: CUDA 12.x not found; build may use system CUDA'"
      ],
      "timeout": "45m"
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Verifying NVIDIA drivers (pre-installed in DLAMI)...'",
        "if command -v nvidia-smi &> /dev/null; then",
        "  echo 'NVIDIA drivers present (from DLAMI)'",
        "  nvidia-smi --list-gpus 2>/dev/null || echo 'No GPUs on build instance (expected)'",
        "else",
        "  echo 'WARNING: NVIDIA drivers not found'",
        "fi"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Installing Python {{user `python_version`}} and build deps...'",
        "sudo dnf install -y python{{user `python_version`}} python{{user `python_version`}}-devel python{{user `python_version`}}-pip",
        "sudo dnf install -y gcc gcc-c++ make git wget jq",
        "python{{user `python_version`}} --version"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Installing FRP client...'",
        "FRP_VERSION=0.65.0",
        "mkdir -p /tmp/frp && cd /tmp/frp",
        "wget -qO- \"https://github.com/fatedier/frp/releases/download/v${FRP_VERSION}/frp_${FRP_VERSION}_linux_amd64.tar.gz\" | tar -xzf -",
        "sudo install -m 0755 \"frp_${FRP_VERSION}_linux_amd64/frpc\" /usr/bin/frpc",
        "rm -rf /tmp/frp",
        "frpc -v"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Creating app directory and venv...'",
        "sudo mkdir -p /app/vllm-poc",
        "sudo chown -R ec2-user:ec2-user /app",
        "mkdir -p /var/tmp/pip-build",
        "chmod 1777 /var/tmp/pip-build",
        "cd /app/vllm-poc",
        "python{{user `python_version`}} -m venv .venv",
        "source .venv/bin/activate",
        "pip install --upgrade pip",
        "echo 'Venv created'"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Preparing vLLM source directory...'",
        "mkdir -p /tmp/vllm-src"
      ]
    },
    {
      "type": "file",
      "source": "vllm",
      "destination": "/tmp/vllm-src/vllm"
    },
    {
      "type": "file",
      "source": "setup.py",
      "destination": "/tmp/vllm-src/setup.py"
    },
    {
      "type": "file",
      "source": "pyproject.toml",
      "destination": "/tmp/vllm-src/pyproject.toml"
    },
    {
      "type": "file",
      "source": "MANIFEST.in",
      "destination": "/tmp/vllm-src/MANIFEST.in"
    },
    {
      "type": "file",
      "source": "README.md",
      "destination": "/tmp/vllm-src/README.md"
    },
    {
      "type": "file",
      "source": "csrc",
      "destination": "/tmp/vllm-src/csrc"
    },
    {
      "type": "file",
      "source": "requirements",
      "destination": "/tmp/vllm-src/requirements"
    },
    {
      "type": "file",
      "source": "CMakeLists.txt",
      "destination": "/tmp/vllm-src/CMakeLists.txt"
    },
    {
      "type": "file",
      "source": "cmake",
      "destination": "/tmp/vllm-src/cmake"
    },
    {
      "type": "file",
      "source": "ami/scripts/vllm-build-with-cache.sh",
      "destination": "/tmp/vllm-build-with-cache.sh"
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Installing AWS CLI (for S3 cache when vllm_build_cache_bucket is set)...'",
        "sudo dnf install -y aws-cli || true",
        "aws --version 2>/dev/null || echo 'aws-cli not installed; S3 cache will be skipped'"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Installing PyTorch and build dependencies (required before pip install)...'",
        "cd /app/vllm-poc",
        "source .venv/bin/activate",
        "export TMPDIR=/var/tmp/pip-build",
        "export TMP=/var/tmp/pip-build",
        "export PIP_DEFAULT_TIMEOUT=300",
        "export PIP_TIMEOUT=300",
        "pip install numpy",
        "pip install torch --index-url https://download.pytorch.org/whl/cu121 || pip install torch",
        "pip install -r /tmp/vllm-src/requirements/build.txt",
        "echo 'Build dependencies installed'"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "export VLLM_BUILD_CACHE_BUCKET='{{user `vllm_build_cache_bucket`}}'",
        "export VLLM_BUILD_CACHE_PREFIX='{{user `vllm_build_cache_prefix`}}'",
        "export VLLM_BUILD_CACHE_VERSION='{{user `vllm_build_cache_version`}}'",
        "export VLLM_BUILD_OVERLAY_FILES='{{user `vllm_build_overlay_files`}}'",
        "export VLLM_BUILD_VERSION='{{user `vllm_build_version`}}'",
        "export VLLM_BUILD_VERBOSE='{{user `vllm_build_verbose`}}'",
        "export PYVER='{{user `python_version`}}'",
        "if [ -n \"$VLLM_BUILD_CACHE_BUCKET\" ]; then echo 'vLLM build cache: ENABLED (bucket='$VLLM_BUILD_CACHE_BUCKET' prefix='${VLLM_BUILD_CACHE_PREFIX:-vllm-wheels}')'; else echo 'vLLM build cache: DISABLED (vllm_build_cache_bucket not set)'; fi",
        "echo 'Installing vLLM (from S3 cache if bucket set, else full build; with cache: intermediate state uploaded every 10 min)...'",
        "export CUDA_HOME=''",
        "for d in /usr/local/cuda-12.1 /usr/local/cuda-12.2 /usr/local/cuda-12.4 /usr/local/cuda-12; do [ -x \"$d/bin/nvcc\" ] 2>/dev/null && export CUDA_HOME=$d && break; done",
        "[ -z \"$CUDA_HOME\" ] && for d in /usr/local/cuda-12*; do [ -x \"$d/bin/nvcc\" ] 2>/dev/null && export CUDA_HOME=$d && break; done",
        "[ -z \"$CUDA_HOME\" ] && export CUDA_HOME=/usr/local/cuda",
        "echo \"Building vLLM with CUDA_HOME=$CUDA_HOME\"",
        "export PATH=$CUDA_HOME/bin:$PATH",
        "export LD_LIBRARY_PATH=$CUDA_HOME/lib64:${LD_LIBRARY_PATH:-}",
        "export TMPDIR=/var/tmp/pip-build",
        "export TMP=/var/tmp/pip-build",
        "cd /app/vllm-poc",
        "source .venv/bin/activate",
        "bash /tmp/vllm-build-with-cache.sh",
        "rm -rf /tmp/vllm-src /tmp/vllm-build-with-cache.sh"
      ],
      "timeout": "180m"
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Verifying vLLM and PoC import...'",
        "cd /app/vllm-poc",
        "source .venv/bin/activate",
        "export VLLM_USE_V1=0",
        "python -c \"import vllm; print('vllm:', vllm.__file__)\"",
        "python -c \"import vllm.poc.routes; print('vllm.poc.routes OK')\"",
        "python -m vllm.entrypoints.openai.api_server --help | head -5",
        "echo 'Verification OK'"
      ]
    },
    {
      "type": "file",
      "source": "ami/start.sh",
      "destination": "/var/tmp/vllm-poc-start.sh"
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Setting up start script and env...'",
        "sudo mv /var/tmp/vllm-poc-start.sh /usr/local/bin/start-vllm-poc.sh",
        "sudo chmod +x /usr/local/bin/start-vllm-poc.sh",
        "sudo tee /etc/profile.d/gonka-vllm-poc.sh > /dev/null <<'EOF'",
        "# Gonka vLLM PoC Environment (native, no Docker)",
        "export VLLM_USE_V1=0",
        "export HF_HOME=${HF_HOME:-/home/ec2-user/.cache/huggingface}",
        "export PATH=\"/app/vllm-poc/.venv/bin:$PATH\"",
        "EOF",
        "sudo chmod 644 /etc/profile.d/gonka-vllm-poc.sh",
        "echo 'Start script: /usr/local/bin/start-vllm-poc.sh'"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'Creating systemd service for vLLM PoC...'",
        "sudo tee /etc/systemd/system/vllm-poc.service > /dev/null <<'EOF'",
        "[Unit]",
        "Description=vLLM PoC Backend (native)",
        "After=network.target",
        "",
        "[Service]",
        "Type=simple",
        "User=ec2-user",
        "WorkingDirectory=/app/vllm-poc",
        "Environment=\"PATH=/app/vllm-poc/.venv/bin:/usr/local/bin:/usr/bin:/bin\"",
        "EnvironmentFile=-/etc/profile.d/gonka-vllm-poc.sh",
        "EnvironmentFile=-/etc/gonka-container.env",
        "ExecStart=/usr/local/bin/start-vllm-poc.sh",
        "Restart=on-failure",
        "RestartSec=10",
        "StandardOutput=journal",
        "StandardError=journal",
        "",
        "[Install]",
        "WantedBy=multi-user.target",
        "EOF",
        "sudo systemctl daemon-reload",
        "echo 'Service: vllm-poc.service'"
      ]
    },
    {
      "type": "shell",
      "inline": [
        "echo 'AMI build completed successfully!'",
        "echo 'Native vLLM + PoC at /app/vllm-poc/.venv (no Docker)'",
        "echo 'Start: /usr/local/bin/start-vllm-poc.sh'",
        "echo 'API: /api/v1/state, /api/v1/pow/init/generate, /api/v1/pow/generate, /api/v1/pow/status, /api/v1/pow/stop'"
      ]
    }
  ]
}
